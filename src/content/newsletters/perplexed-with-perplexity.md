---
title: Perplexed with Perplexity
lastUpdatedDate: 2024-07-17 01:40:42.620904+00:00
publicationDate: 2024-06-30 01:40:42.620904+00:00
season: 7
---

Hello frens,

I no longer feel comfortable using [Perplexity](https://www.perplexity.ai).

You may remember I was [trialing it a few months ago](https://rwblickhan.org/newsletters/the-old-cyberpunk-vision-of-a-world-of-neoliberal-corporations-run-amok/#other-stuff). In many ways, itâ€™s an impressive product, combining an LLM-powered chatbot with a search index â€” ask a question, and itâ€™ll come back with a specific answer and citations to websites that itâ€™s pulling from. In the example above, I asked for the San Francisco city budget, and it immediately spat out both the correct answer and the PDF from the city that included it.

Recently, however, theyâ€™ve made a number of decisions that leave me feeling... queasy.

The Verge has a [fantastic summary](https://www.theverge.com/2024/6/27/24187405/perplexity-ai-twitter-lie-plagiarism) of the situation, but the short version is:

- Perplexity [rolled out Pages](https://www.perplexity.ai/hub/blog/perplexity-pages), a *curious* feature that generates a shareable page of â€œresearchâ€ about a topic based on a user prompt. In other words, Perplexity can now write shoddy Medium articles. Iâ€™m just as perplexed [as the Verge was](https://www.theverge.com/2024/5/30/24167986/perplexity-ai-research-pages-school-report) when they reviewed it; their judgement that itâ€™s only useful for â€œstudents rushing to put out an assignmentâ€ seems downright charitable.
- Forbes realized that Perplexity was [able to circumvent their paywall](https://www.forbes.com/sites/sarahemerson/2024/06/07/buzzy-ai-search-engine-perplexity-is-directly-ripping-off-content-from-news-outlets/) to summarize articles for Pages, accusing the company of copyright infringement.
- Wired discovered that Perplexity was [accessing their site via an unlisted IP address](https://www.wired.com/story/perplexity-is-a-bullshit-machine/), even though Wired had set up `robots.txt` to block Perplexityâ€™s crawler per their instructions.
- In response, Perplexityâ€™s CEO claimed [they were relying on a third-party web crawler](https://www.fastcompany.com/91144894/perplexity-ai-ceo-aravind-srinivas-on-plagiarism-accusations) that wasnâ€™t respecting `robots.txt`, but refused to name the third party or commit to asking them to stop crawling Wiredâ€™s content.
- 404 Media published that [Perplexityâ€™s origin story](https://www.404media.co/perplexitys-origin-story-scraping-twitter-with-fake-academic-accounts/) involved a demo of a research tool powered by the somewhat dubious process of scraping Twitter with fake, AI-generated academic accounts.

So, cards on the table: I generally disagree that LLM *training* is a form of plagiarism, even when performed on unlicensed copyrighted material, as virtually all production language models are. Mashing up â€œthe entire contents of the Internet and several sizable librariesâ€ into a set of vectors and then asking for a probabilistic continuation feels meaningfully different than, say, uploading a copyrighted text to Annaâ€™s Archive â€” even if a clever choice of prompt causes that probabilistic continuation to align very closely with â€œactualâ€ writing out there in the world.

Aside: Some writers compare LLM training to human education â€” if a human can read and be influenced by their favorite authors, why not an LLM? Iâ€™m intentionally not doing so here since, as [Robin Sloan has pointed out](https://www.robinsloan.com/lab/at-home-in-high-dimensional-space/#training), we shouldnâ€™t take the â€œlearningâ€ analogy too far. Humans across their lifetimes can barely read a few drops of water in the ocean of content that LLMs swim in; itâ€™s simply not comparable. Also, invoking â€œeducationâ€ brings us dangerously close to the territory of consciousness, and weâ€™ll soon be barraged with stochastic parrots and Chinese rooms, which I as an [illusionist about consciousness](https://keithfrankish.github.io/articles/Frankish_Illusionism%20as%20a%20theory%20of%20consciousness_eprint.pdf) simply disagree with ğŸ™‚

In any case, Iâ€™m confident enough in the probabilistic take on LLMs â€” that they generate â€œaverageâ€, but net new, sentences â€” that I have no particular qualms about using LLMs, nor am I rushing to defend my website from scraping. I could probably talk more about this another time, but the point Iâ€™m getting at is that Iâ€™m reasonably comfortable with LLMs *as a technology*.

What Perplexity is doing, however, feels different. It feels *sketchy*.

There is a reasonable argument that Perplexity â€œshouldâ€ be allowed to ignore `robots.txt`, if the user explicitly asked for it to retrieve a webpage. Itâ€™s not entirely clear that thatâ€™s whatâ€™s happening here, but even if it is, Nick Heer [ably demonstrates](https://pxlnv.com/blog/on-robots-and-text/) why that isnâ€™t valid: â€œA webpage being rendered through Perplexity is actually being reinterpreted and modified. The original text of the page is transformed through automated means about which neither the reader or the publisher has any understanding,â€ which a reasonable publisher may very well object to. Heer concludes that â€œ\[t\]he absolute least Perplexity can do is respecting those objections by clearly and consistently identifying itself, and excluding websites which have indicated they do not want to be accessed by these means.â€[^quotes]

That Perplexityâ€™s automated access skips the publisherâ€™s paywall or ads in return for a paltry number of pageviews is really just the cherry on the pie. As a non-journalist, my paycheck doesnâ€™t depend on Perplexityâ€™s actions, but I understand why theyâ€™re burning with rage at Perplexity and Arc Search and Googleâ€™s AI foibles â€” theyâ€™re an automated version of the worst of the worst of the old link aggregators.

I liked Perplexity as a genuine research tool, cutting through the sea of Google links to find the site that can actually answer my question. That was always tenuous â€” after all, Perplexity has always provided answers directly, and I suspect very few people clicked the inline citations like I did â€” but if Perplexity was previously tenuous, Pages seems like an outright [slop](https://simonwillison.net/2024/May/8/slop/) factory based entirely on plagiarism. The CEOâ€™s cagey response to Wiredâ€™s accusations certainly doesnâ€™t inspire confidence.

I suppose I land [somewhat close to Ernie Smith](https://tedium.co/2024/06/20/perplexity-forbes-ai-aggregation-risks/): cautiously optimistic that LLMs will still be useful, but only if we choose to hold them to higher standards.

Talk again soon,\
Russell

P.S. I know I said this would come in two weeks, but Iâ€™m bored and this seemed topical. I suspect newsletters will come more often for the next few weeks.

P.P.S. I wrote up [what apps Iâ€™ve been using recently](https://rwblickhan.org/technical/2024-apps/). Iâ€™m not sure if anybody gets value out of lists like these, but I like reading them when other people post ğŸ¤·â€â™€ï¸

[^quotes]: I note the irony that Iâ€™m quoting Heer at length with a link for context, not all that different than Perplexity is doing. The difference, of course, is that Iâ€™m a regular reader that sees every sponsored post and I currently have the site open to copy-paste quotes.